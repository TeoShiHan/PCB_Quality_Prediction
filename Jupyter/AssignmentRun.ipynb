{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Machine1(Oct 2020) and Machine2(Aug 2020) datasets\n",
    "\n",
    "MC1csv = pd.read_csv(\"MC2Oct2020_csv.csv\")\n",
    "\n",
    "MC2csv = pd.read_csv(\"MC2Aug2020_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73046ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "MC1 = MC1csv.drop(columns=['Part', 'IC 2D','Lot','Date','Time','Machine','Server Result'], axis=1)\n",
    "MC2 = MC2csv.drop(columns=['Part', 'IC 2D','Lot','Date','Time','Machine','Server Result'], axis=1)\n",
    "MC1['Machine Result']=le.fit_transform(MC1['Machine Result'])\n",
    "MC2['Machine Result']=le.fit_transform(MC2['Machine Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1MC1 = MC1.quantile(0.25)\n",
    "Q3MC1 = MC1.quantile(0.75)\n",
    "IQR1 = Q3MC1-Q1MC1\n",
    "\n",
    "MC1mean = MC1.mean()\n",
    "MC1median = MC1.median()\n",
    "\n",
    "MC1[(MC1<(Q1MC1 - 1.5 * IQR1))|(MC1>(Q3MC1 + 1.5 * IQR1))]=np.nan\n",
    "MC1Mean = MC1.fillna(MC1mean) \n",
    "MC1Median = MC1.fillna(MC1median)\n",
    "MC1Abs = (MC1.fillna(MC1mean).abs()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497b308",
   "metadata": {},
   "source": [
    "## Normalizing MC1Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X = MC1Mean.drop('Machine Result', axis=1)\n",
    "y = le.fit_transform(MC1Mean['Machine Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a2567",
   "metadata": {},
   "source": [
    "## KNN MC1Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"Before tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'weights': ['uniform', 'distance'],'n_neighbors': list(np.arange(1, 100, 1))}] \n",
    "\n",
    "grid = GridSearchCV(knn, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"After tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bece18",
   "metadata": {},
   "source": [
    "## GaussianNB MC1Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"Before tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'var_smoothing': np.logspace(0,-9, num=100) \n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(gnb, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aed9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"After tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6be70",
   "metadata": {},
   "source": [
    "## RandomForestClassifier MC1Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"Before tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators':list(np.arange(1, 100, 1))\n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(rfc, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08949d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"After tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fac24",
   "metadata": {},
   "source": [
    "## Normalizing MC1Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a609ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X = MC1Median.drop('Machine Result', axis=1)\n",
    "y = le.fit_transform(MC1Median['Machine Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc21c52",
   "metadata": {},
   "source": [
    "## KNN MC1Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d22002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"Before tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'weights': ['uniform', 'distance'],'n_neighbors': list(np.arange(1, 100, 1))}] \n",
    "\n",
    "grid = GridSearchCV(knn, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a455714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"After tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de113ba",
   "metadata": {},
   "source": [
    "## GaussianNB MC1Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"Before tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'var_smoothing': np.logspace(0,-9, num=100) \n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(gnb, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27238081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"After tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a858587",
   "metadata": {},
   "source": [
    "## RandomForestClassifier MC1Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4cbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"Before tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72739268",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators':list(np.arange(1, 100, 1))\n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(rfc, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"After tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8b00a",
   "metadata": {},
   "source": [
    "## Normalizing MC1Abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X = MC1Abs.drop('Machine Result', axis=1)\n",
    "y = le.fit_transform(MC1Abs['Machine Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e206d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467409e8",
   "metadata": {},
   "source": [
    "## KNN MC1Abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cac8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"Before tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4877fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'weights': ['uniform', 'distance'],'n_neighbors': list(np.arange(1, 100, 1))}] \n",
    "\n",
    "grid = GridSearchCV(knn, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "knn = KNeighborsClassifier(n_neighbors=16, weights='distance')\n",
    "knn.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = knn.predict(Xtest)\n",
    "print(\"After tuning KNN\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0d5de",
   "metadata": {},
   "source": [
    "## GaussianNB MC1Abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e235891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"Before tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'var_smoothing': np.logspace(0,-9, num=100) \n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(gnb, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8652cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = gnb.predict(Xtest)\n",
    "print(\"After tuning GaussianNB\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1d1b6",
   "metadata": {},
   "source": [
    "## RandomForestClassifier MC1Abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"Before tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0853c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators':list(np.arange(1, 100, 1))\n",
    "              }] \n",
    "\n",
    "grid = GridSearchCV(rfc, parameters)\n",
    "grid.fit(Xtrain, ytrain)\n",
    " \n",
    "print(grid.best_estimator_) #print how the model looks after hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "ypred = rfc.predict(Xtest)\n",
    "print(\"After tuning RandomForestClassifier\")\n",
    "print(\"Mean squared error:\", mean_squared_error(ytest, ypred))\n",
    "print(\"Accuracy score:\", accuracy_score(ytest, ypred))\n",
    "print(\"Precision score: \", precision_score(ytest, ypred))\n",
    "print(\"Recall score: \", recall_score(ytest, ypred)) #balance between precision and recall\n",
    "print(\"F1 score: \", f1_score(ytest, ypred))\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
